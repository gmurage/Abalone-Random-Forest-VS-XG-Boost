{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef1ad3f-40be-48a3-9aad-e1a802637c4f",
   "metadata": {},
   "source": [
    "\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "   \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "                                          \n",
    "#                                                 Regression with Abalone Data Set: Random Forest and XG BOOST Models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                                                        Gladys Murage\n",
    "\n",
    "#                              College of Business, Engineering, and  Technology, National University\n",
    "\n",
    "#                                         DDS8555 v1: PREDICTIVE ANALYSIS(3602869492)\n",
    "\n",
    "#                                                        Dr MOHAMED NABEEL\n",
    "\n",
    "#                                                            March 06, 2025\n",
    "\n",
    "\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "<BR>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6f523-bae2-4082-b38e-1ffdccaebcd2",
   "metadata": {},
   "source": [
    "# Model 2 is Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b752da1-4994-45f1-853b-12694a087428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE (Random Forest): 0.15567026949474086\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "# Identify categorical columns (assuming they are object dtype)\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "# Remove the target variable 'Rings' and apply one-hot encoding\n",
    "X = pd.get_dummies(train_data.drop('Rings', axis=1))  # Keep this line\n",
    "y = train_data['Rings']\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Initialize and train Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred_rf = rf_model.predict(X_val_scaled)\n",
    "\n",
    "# Define RMSLE function\n",
    "def rmsle(y_true, y_pred):\n",
    "    import numpy as np\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2)))\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "rmsle_value_rf = rmsle(y_val, y_val_pred_rf)\n",
    "print(f\"Validation RMSLE (Random Forest): {rmsle_value_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf8dc5-7b0b-45f6-b857-86fa4517b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_data[\"id\"],  \n",
    "    \"Rings\": y_pred\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(\"submission2.csv\", index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456e91b-da90-4af0-b1fa-707d3c49e246",
   "metadata": {},
   "source": [
    "# Results with Random Forest RMSLE = 0.1557\n",
    "This shows some improvement but a better model to evaluate RMSLE is called for. I shall now perform XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdff382-f315-4acd-9906-501dc1e6be96",
   "metadata": {},
   "source": [
    "# XGBoost is a powerful gradient boosting algorithm\n",
    "XG Boost is an algorithim that often outperforms Random Forest and Linear Regression, especially in structured data problems. Below is the code incorporating XGBoost with hyperparameter tuning.\n",
    "\n",
    "# Why XGBoost?\n",
    "✅ Handles missing values automatically\n",
    "✅ No need for feature scaling\n",
    "✅ Often outperforms Random Forest in structured data\n",
    "✅ Supports parallelization and GPU acceleration\n",
    "\n",
    "# Key Features in This Code\n",
    "1. Raw Features Used: XGBoost does not require feature scaling.\n",
    "2. Hyperparameter Tuning: Uses GridSearchCV for optimal parameters.\n",
    "3. Log-Transformation Option: Helps with target variable skewness.\n",
    "4. Expected Outcome:\n",
    "\n",
    "    a) A Likely lower RMSLE compared to Linear Regression and Random Forest.\n",
    "\n",
    "    b) Faster training with optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0076a054-a956-47e9-b140-fa5956636575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: xgboost in ./.local/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.local/lib/python3.10/site-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages (from xgboost) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d31495-e0b3-4823-8a52-339844314c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSLE (XGBoost): 0.1531\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Validation RMSLE (Best XGBoost Model): 0.1521\n",
      "Validation RMSLE (Log-Transformed Target, XGBoost): 0.1514\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define RMSLE function\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Logarithmic Error (RMSLE).\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred, 0)  # Ensure non-negative predictions\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Handle missing values (better than dropping all rows)\n",
    "# Fill missing values only for numeric columns\n",
    "numeric_cols = train_data.select_dtypes(include=['number']).columns\n",
    "train_data[numeric_cols] = train_data[numeric_cols].fillna(train_data[numeric_cols].median())\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "train_data = pd.get_dummies(train_data, columns=[\"Sex\"], drop_first=True)\n",
    "\n",
    "# Separate features and target\n",
    "X = train_data.drop(\"Rings\", axis=1)\n",
    "y = train_data[\"Rings\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardization is NOT needed for XGBoost, but we can try both ways\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = XGBRegressor(objective=\"reg:squarederror\", random_state=42, n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)  # Using raw data (XGBoost handles scaling internally)\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate XGBoost using RMSLE\n",
    "rmsle_value_xgb = rmsle(y_val, y_val_pred_xgb)\n",
    "print(f\"Validation RMSLE (XGBoost): {rmsle_value_xgb:.4f}\")\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search_xgb = GridSearchCV(XGBRegressor(objective=\"reg:squarederror\", random_state=42),\n",
    "                               param_grid_xgb, scoring=\"neg_mean_squared_log_error\", cv=5, verbose=1)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best XGBoost model\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_val_pred_best_xgb = best_xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the best XGBoost model using RMSLE\n",
    "rmsle_value_best_xgb = rmsle(y_val, y_val_pred_best_xgb)\n",
    "print(f\"Validation RMSLE (Best XGBoost Model): {rmsle_value_best_xgb:.4f}\")\n",
    "\n",
    "# Log Transformation of the Target (Optional)\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_val_log = np.log1p(y_val)\n",
    "\n",
    "# Train and evaluate the XGBoost model with log-transformed target\n",
    "best_xgb_model.fit(X_train, y_train_log)\n",
    "y_val_pred_log_xgb = best_xgb_model.predict(X_val)\n",
    "y_val_pred_xgb_final = np.expm1(y_val_pred_log_xgb)\n",
    "\n",
    "# Evaluate transformed model\n",
    "rmsle_value_log_xgb = rmsle(y_val, y_val_pred_xgb_final)\n",
    "print(f\"Validation RMSLE (Log-Transformed Target, XGBoost): {rmsle_value_log_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586140e8-3d22-4fff-b9bd-a26c042b3e84",
   "metadata": {},
   "source": [
    "# Prepare the Submission File\n",
    "Format the predictions to match the sample submission file. Typically, the submission file requires an ID column and the predicted target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2060e41e-d20b-446d-9e39-12695afce55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 3 file created successfully!\n"
     ]
    }
   ],
   "source": [
    "## Create a submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_data[\"id\"],  # Assuming the test data has an 'id' column\n",
    "    \"Rings\": y_pred\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "print(\"Submission 3 file created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b4145-b521-413d-b398-2b09b0e9d34d",
   "metadata": {},
   "source": [
    "# Verify the Submission File\r\n",
    "Ensure the submission file matches the format of the sample submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8a2fddf-01cc-49ea-9303-1838379f904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id      Rings\n",
      "0  90615   9.930454\n",
      "1  90616   9.582545\n",
      "2  90617  10.162455\n",
      "3  90618  10.875250\n",
      "4  90619   7.533986\n"
     ]
    }
   ],
   "source": [
    "# Load the submission file into a DataFrame\n",
    "submission3 = pd.read_csv(\"submission3.csv\")\n",
    "\n",
    "# Print the top 5 records\n",
    "print(submission3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45277ef3-952e-4ffa-a44c-30884560ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id      Rings\n",
      "60406  151021   6.427125\n",
      "60407  151022   9.437661\n",
      "60408  151023  12.048299\n",
      "60409  151024  13.385188\n",
      "60410  151025   8.682507\n"
     ]
    }
   ],
   "source": [
    "# Print the last 5 records\n",
    "print(submission3.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20bd89a4-e61d-488d-8a24-c1fde38e4c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of submission file: (60411, 2)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the submission 3 DataFrame\n",
    "print(\"Shape of submission file:\", submission3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823beb0-631b-4a75-ad25-2e9f52dd65de",
   "metadata": {},
   "source": [
    "# The results obtained show the Validation RMSLE for multiple models and steps in the workflow. \n",
    "A detailed interpretation of these results and what they mean for the model's performance:\n",
    "\n",
    "# 1. Summary of Results\n",
    "Model / Step\tValidation RMSLE\n",
    "\n",
    "Linear Regression\t0.1668\n",
    "\n",
    "Random Forest\t0.1557\n",
    "\n",
    "XGBoost (Default Hyperparameters)\t0.1531\n",
    "\n",
    "XGBoost (Best Model from Grid Search)\t0.1521\n",
    "\n",
    "XGBoost (Log-Transformed Target)\t0.1514\n",
    "\n",
    "# 2. Interpretation of Results\n",
    "## Linear Regression (RMSLE: 0.1668)\n",
    "The Linear Regression model has the highest RMSLE, indicating that it is the least accurate among the models tested.\n",
    "\n",
    "This is expected because Linear Regression is a simple model and may not capture complex patterns in the data.\n",
    "\n",
    "## Random Forest (RMSLE: 0.1557)\n",
    "The Random Forest model performs better than Linear Regression, with a lower RMSLE of 0.1557.\n",
    "\n",
    "Random Forest is more flexible and can capture non-linear relationships in the data, which explains the improvement.\n",
    "\n",
    "## XGBoost (Default Hyperparameters, RMSLE: 0.1531)\n",
    "The XGBoost model with default hyperparameters has a lower RMSLE (0.1531) than Random Forest, making it the best-performing model so far.\n",
    "\n",
    "XGBoost is a powerful ensemble method that often outperforms Random Forest due to its gradient-boosting framework and regularization techniques.\n",
    "\n",
    "## XGBoost (Best Model from Grid Search, RMSLE: 0.1521)\n",
    "After hyperparameter tuning using Grid Search, the best XGBoost model achieves a slightly lower RMSLE of 0.1521.\n",
    "\n",
    "This improvement shows that hyperparameter tuning can help optimize the model's performance.\n",
    "\n",
    "## XGBoost (Log-Transformed Target, RMSLE: 0.1514)\n",
    "Applying a log transformation to the target variable further reduces the RMSLE to 0.1514.\n",
    "\n",
    "Log transformation helps handle skewness in the target variable, making the model more robust to extreme values.\n",
    "\n",
    "# 3. Key Insights\n",
    "## a) Model Complexity Matters:\n",
    "\n",
    "More complex models (Random Forest, XGBoost) outperform simpler models (Linear Regression) because they can capture non-linear relationships in the data.\n",
    "\n",
    "## b) Hyperparameter Tuning Helps:\n",
    "\n",
    "Grid Search improves the XGBoost model's performance by finding the best hyperparameters.\n",
    "\n",
    "## C) Log Transformation Improves Performance:\n",
    "\n",
    "Log-transforming the target variable reduces skewness and further improves the model's performance.\n",
    "\n",
    "## d) Room for Improvement:\n",
    "\n",
    "While the RMSLE has improved significantly, there is still room for further improvement (e.g., feature engineering, advanced models). \n",
    "\n",
    "# 4. Next Steps\n",
    "## Step 1: Feature Engineering\n",
    "Create new features (e.g., interaction terms, polynomial features).\n",
    "\n",
    "Transform features (e.g., log transformation, scaling).\n",
    "\n",
    "## Step 2: Try Advanced Models\n",
    "LightGBM: A faster and more efficient gradient-boosting framework.\n",
    "\n",
    "CatBoost: Handles categorical features natively and is robust to overfitting.\n",
    "\n",
    "Neural Networks: For highly complex datasets.\n",
    "\n",
    "## Step 3: Ensemble Methods\n",
    "Combine multiple models (e.g., XGBoost, Random Forest) to improve performance.\n",
    "\n",
    "## Step 4: Handle Outliers\n",
    "Identify and handle outliers in the target variable or features.\n",
    "\n",
    "## Step 5: Cross-Validation\n",
    "Use cross-validation to ensure the model's performance is consistent across different subsets of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
